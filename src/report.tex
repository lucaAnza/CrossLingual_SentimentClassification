\documentclass[11pt]{article}

% ---------- Packages ----------
\usepackage[a4paper,margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{todonotes}
\definecolor{lightgreen}{RGB}{220,245,220}  % Used for table highlights

% ---------- Title ----------
\title{\textbf{Sentiment Analysis of Multilingual Amazon Reviews}\\
AI509/508/DM894 Final Project Report}
\author{Imanshu Sharma \and Alberto Aiello \and Luca Anzaldi}
\date{\today}

\begin{document}
\maketitle

% Optional (not required by template, but commonly useful)
\begin{abstract}
This project focuses on sentiment analysis of Amazon product reviews in multiple languages.
The goal is to train and evaluate neural language models to predict the sentiment expressed in
customer reviews.
We analyze how different factors influence model performance, including the amount of
training data available for each language and the use of multilingual versus monolingual
models.
In particular, we study (i) how performance changes as the size of the training data increases,
(ii) whether a single multilingual model can effectively handle multiple languages, and
(iii) how performance varies across languages.
Our experiments use a compact multilingual transformer model and provide a baseline evaluation
of sentiment prediction on Amazon reviews.
\end{abstract}

\newpage
\newpage
% =========================================================
% 1. Introduction and Problem Presentation
% =========================================================
\section{Introduction and Problem Presentation}

\subsection{Task description}
Given an Amazon review text, the goal is to predict its sentiment.
We consider two related formulations:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Classification:} predict discrete sentiment labels (e.g., negative / neutral / positive or 1--5 stars).
    \item \textbf{Regression:} predict a continuous sentiment score derived from the 1--5 star rating.
\end{itemize}
From an NLP perspective, this task requires transforming raw review text into meaningful numerical
representations that can be processed by neural models.
Each review is first tokenized into subword units and mapped into embeddings that capture semantic
information.
These representations are then processed by transformer-based architectures, which use attention
mechanisms to model relationships between words and capture sentiment cues across the entire review.
This approach allows the model to handle long texts, negation, and context-dependent sentiment,
which are common in customer reviews.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{Taskdescription.png}
    \caption{End-to-end sentiment analysis pipeline.
    An Amazon review is processed by the model tokenizer, mapped to embeddings,
    encoded by a transformer model, and passed to a task-specific prediction head.}
    \label{fig:sentiment_pipeline}
\end{figure}

\subsection{Project goal and research questions}
Our project investigates the following questions:
\begin{enumerate}[itemsep=2pt]
    \item \textbf{How much target-language data is needed?}
    We vary training set size (e.g., 10\%, 30\%, 50\%, 70\%, 100\%) and identify where performance saturataes.
    \item \textbf{One multilingual model vs. monolingual models: which works better?}
    We compare a shared multilingual model trained on all languages against separate models trained per language.
    \item \textbf{Per-language performance ranking:}
    We compute and compare evaluation metrics for each language to understand which languages are easier/harder.
\end{enumerate}

\subsection{Why the problem is challenging}
\begin{itemize}[itemsep=2pt]
    \item \textbf{Noisy and subjective labels:}
    Star ratings do not always reflect the sentiment expressed in the text.
    Users may assign high ratings while writing neutral reviews, or low ratings for reasons
    that are not directly related to the product quality.

    \item \textbf{Ordinal nature of the target:}
    Sentiment ratings from 1 to 5 stars are ordered, which means that prediction errors have
    different importance.
    For example, predicting 4 instead of 5 stars is less severe than predicting 1 instead of 5,
    which motivates the use of evaluation metrics that take ordering into account.

    \item \textbf{Context-dependent sentiment:}
    The sentiment of a word or sentence often depends on its context.
    Elements such as negation or mixed opinions make it difficult to determine sentiment
    based on individual words, and require models that can capture information across the
    entire review.

    \item \textbf{Multilingual and cross-lingual variability:}
    Reviews are written in different languages, each with its own grammar, vocabulary, and
    writing conventions.
    This creates challenges for tokenization and representation learning, especially when
    using a single model across multiple languages.

    \item \textbf{Domain and style variation:}
    Amazon reviews cover many product categories and writing styles, ranging from very short
    comments to long and detailed descriptions.
    This diversity makes it harder to learn sentiment representations that generalize well.
\end{itemize}


% =========================================================
% 2. Method
% =========================================================
\newpage
\noindent\rule{\linewidth}{0.4pt}
\section{Method}

\subsection{Computing resources (Hardware)}
All experiments were executed on the SDU/University GPU cluster. In the table below are showed the \textit{hardware specification}.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Compute node & \texttt{u1-gpu-h-1} \\
CPU & Intel Xeon Gold 6230 (20 vCPUs) \\
RAM & 46 GB \\
GPU & 1 $\times$ NVIDIA V100 \\
OS / Environment & Linux + Conda \\
\bottomrule
\end{tabular}
\caption{Hardware configuration used for model training and evaluation.}
\label{tab:hardware}
\end{table}

\subsection{Software stack (Frameworks and tools)}
We implemented the pipeline in Python using the following libraries:
\begin{itemize}[itemsep=2pt]
    \item \textbf{PyTorch}: backend for model training and automatic differentiation.
    \item \textbf{Hugging Face Transformers}: model loading, tokenization, and the training loop (\texttt{Trainer}).
    \item \textbf{Hugging Face Datasets}: loading CSV splits and preprocessing with efficient mapping.
    \item \textbf{Evaluate}: metric computation during evaluation.
    \item \textbf{Weights \& Biases (wandb)}: experiment tracking (loss curves, metrics, run metadata).
    \item \textbf{NumPy}: metric utilities and post-processing.
\end{itemize}

\subsection{Dataset overview}
We use a multilingual Amazon Reviews dataset \cite{kaggle_amazon_multilingual} split into \texttt{train.csv}, \texttt{validation.csv}, and \texttt{test.csv}.
Each review includes a 1--5 star rating and textual fields (title/body), along with metadata such as language and product category.
The dataset covers multiple languages (e.g., EN, ES, DE, FR, JA, ZH).

\subsubsection{Data fields}
\begin{table}[H]
\centering
\begin{tabular}{lp{9cm}}
\toprule
\textbf{Field} & \textbf{Description} \\
\midrule
\texttt{review\_id} & Unique identifier of the review. \\
\texttt{product\_id} & Identifier of the product being reviewed. \\
\texttt{reviewer\_id} & Identifier of the user who wrote the review. \\
\texttt{stars} & Integer rating from 1 to 5 stars assigned by the user. \\
\texttt{review\_title} & Short title of the review. \\
\texttt{review\_body} & Main textual content of the review. \\
\texttt{language} & Language in which the review is written. \\
\texttt{product\_category} & Category of the reviewed product. \\
\bottomrule
\end{tabular}
\caption{Overview of the main fields in the Amazon Reviews dataset.}
\label{tab:dataset_fields}
\end{table}

Non-essential metadata (e.g., review IDs, product identifiers, language tags, and product category fields)
are removed for the baseline experiments.




\subsubsection{Label distribution}
The training split is perfectly balanced across the five star ratings, as shown in Table~\ref{tab:label_distribution_train}.

\begin{table}[H]
\centering
\begin{tabular}{cc}
\toprule
\textbf{Star rating} & \textbf{Number of samples} \\
\midrule
1 & 240{,}000 \\
2 & 240{,}000 \\
3 & 240{,}000 \\
4 & 240{,}000 \\
5 & 240{,}000 \\
\bottomrule
\end{tabular}
\caption{Label distribution in the training set (balanced).}
\label{tab:label_distribution_train}
\end{table}

The validation and test sets follow the same balanced distribution, with 6{,}000 samples per star rating.

\subsubsection{Dataset split sizes}
Table~\ref{tab:dataset_splits} summarizes the number of samples used in each split.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Split} & \textbf{Samples per class} & \textbf{Total samples} \\
\midrule
Training & 240{,}000 & 1{,}200{,}000 \\
Validation & 6{,}000 & 30{,}000 \\
Test & 6{,}000 & 30{,}000 \\
\bottomrule
\end{tabular}
\caption{Overview of dataset splits used in the experiments.}
\label{tab:dataset_splits}
\end{table}

This balanced setup allows a fair comparison between regression and classification approaches
and ensures that evaluation metrics are not biased by class imbalance.

\newpage
\subsection{Model choice}
We use \texttt{distilbert-base-multilingual-cased}, an \textbf{encoder-only} Transformer model.
It is a distilled variant of multilingual BERT, providing a good trade-off between computational
efficiency and multilingual coverage \cite{sanh2019distilbert,wolf2020transformers}.\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Feature} & \textbf{Specification} \\
\midrule
Model name & \texttt{distilbert-base-multilingual-cased} \\
Architecture & Encoder-only Transformer (DistilBERT) \\
Number of layers & 6 Transformer encoder layers \\
Hidden size & 768 \\
Attention heads & 12 \\
Total parameters & $\sim$134 million \\
Vocabulary size & $\sim$119{,}547 \\
Maximum sequence length & 512 tokens \\
Pretraining objective & Masked Language Modeling (MLM) \\
Languages covered & 100+ languages \\
Case sensitive & Yes (cased) \\
Disk size (weights) & $\sim$517 MB \\
\bottomrule
\end{tabular}
\caption{Main characteristics of the \texttt{distilbert-base-multilingual-cased} model \cite{hf_distilbert_multilingual}}
\label{tab:model_features}
\end{table}

\subsection{Preprocessing and input representation}
\paragraph{Column selection.}\label{column-selection}
We rename \texttt{stars} to \texttt{label} and construct the input text by concatenating the review title and body
(\texttt{text} = \texttt{review\_title} + \texttt{review\_body}).
Non-essential metadata (e.g., review IDs, product identifiers, language tags, and product category fields)
are removed for the baseline experiments.

\paragraph{Tokenization.}
The input text is processed using the tokenizer associated with the pre-trained model.
This tokenizer applies a subword tokenization strategy, which splits the text into smaller
units that belong to a shared multilingual vocabulary.
Subword tokenization helps handle rare words, spelling variations, and text written in
different languages, and ensures compatibility with the model vocabulary.
Truncation is applied to limit the maximum sequence length, while dynamic padding is used
to efficiently batch inputs during training.



\paragraph{Model vocabulary.}
The model uses a shared multilingual subword vocabulary rather than
separate vocabularies for each language.
This design allows the same tokens or subword units to be reused across
different languages, enabling cross-lingual transfer.
As a result, the model can process reviews written in multiple languages
using a single tokenizer and a unified representation space.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{tokenizexample.png}
    \caption{Example tokenization of Amazon review sentences in different languages
    using the \texttt{distilbert-base-multilingual-cased} tokenizer.
    The figure shows how a shared multilingual subword vocabulary is used across languages.}
    \label{fig:tokenization_examples}
\end{figure}


\paragraph{Label normalization.}
In the \textit{regression} setting, the original star ratings
$y \in \{1,2,3,4,5\}$ are linearly normalized to the interval $[0,1]$:
\[
y_{\text{norm}} = \frac{y - 1}{4}.
\]
For evaluation and interpretability, model predictions are mapped back to the original
rating scale using the inverse transformation:
\[
\hat{y} = 4 \cdot \hat{y}_{\text{norm}} + 1.
\]

In the \textit{classification setting}, star ratings are re-encoded from the original range
$\{1,\dots,5\}$ to $\{0,\dots,4\}$, in accordance with the Hugging Face
\texttt{Transformers} framework, which requires class labels to be zero-indexed.


%%% TRAINING SETUP
\subsection{Training setup}
Training is performed using task-specific loss functions.
For the \textit{regression formulation}, we minimize the \textbf{Mean Squared Error (MSE)} loss between
predicted and ground-truth star ratings.
For the \textit{classification formulation}, we optimize the \textbf{categorical cross-entropy} loss,
as standard for multi-class classification in the Hugging Face \texttt{Transformers} framework.

%Table with settings
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Training batch size & 32 \\
Gradient accumulation steps & 4 \\
Effective batch size & 128 \\
Number of epochs & 8 \\
Weight decay & 0.01 \\
Learning rate scheduler & Cosine \\
Warmup ratio & 0.06 \\
Maximum gradient norm & 1.0 \\
\bottomrule
\end{tabular}
\caption{Key training hyperparameters used in the experiments.}
\label{tab:training_hyperparameters}
\end{table}

% Hyperparams motivation
\paragraph{Hyperparameter selection.}
\begin{itemize}[itemsep=2pt]
    \item The training batch size was set to 32 to ensure stable optimization while avoiding GPU memory constraints.
    \item Gradient accumulation with four steps was used to simulate a larger effective batch size of 128 without increasing memory usage.
    \item The model was trained for eight epochs to allow sufficient convergence while limiting the risk of overfitting.
    \item Weight decay was applied to regularize model parameters and mitigate overfitting on noisy sentiment labels.
    \item A cosine learning rate scheduler with a warmup ratio of 0.06 was adopted to ensure smooth learning rate decay and stable early training.
    \item Gradient clipping with a maximum norm of 1.0 was employed to prevent exploding gradients and improve overall training stability.
\end{itemize}


\subsection{Evaluation metrics}
Evaluation is performed using different metrics depending on whether the task is formulated
as \textbf{regression} or \textbf{classification}.

\subsubsection{Regression setting.}
When modeling sentiment as a continuous variable, we report the following metrics:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Mean Absolute Error (MAE)} and \textbf{Root Mean Squared Error (RMSE)}, computed in the original 1--5 star scale, to measure average prediction error.
    \item \textbf{Rounded Accuracy (5-class)}, obtained by rounding the continuous prediction to the nearest integer star rating.
    \item \textbf{Rounded Accuracy (3-class)}, computed after mapping 1--2$\rightarrow$negative, 3$\rightarrow$neutral, and 4--5$\rightarrow$positive.
    \item \textbf{Spearman’s rank correlation coefficient ($\rho$)}, to assess how well the model preserves the ordinal ranking of sentiment.
    \item \textbf{Quadratic Weighted Kappa (QWK)}, which measures ordinal agreement while penalizing larger rating discrepancies more heavily.
\end{itemize}

\subsubsection{Classification setting.}
When sentiment prediction is formulated as a classification task, we report:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Accuracy (5-class)} and \textbf{Macro F1-score (5-class)}, to evaluate exact label prediction performance across all star ratings.
    \item \textbf{Quadratic Weighted Kappa (QWK)}, to account for the ordinal structure of the 1--5 star labels.
    \item \textbf{Accuracy (3-class)} and \textbf{Macro F1-score (3-class)}, computed after mapping ratings to negative, neutral, and positive sentiment.
\end{itemize}


\subsection{Experiment protocol}

\subsubsection{Regression vs Classification}
The first set of experiments was designed to assess whether sentiment prediction should be
formulated as a \textit{regression} or a \textit{classification} problem.
Since the Amazon review ratings are ordinal in nature, both approaches are viable:
classification treats each star rating as a discrete category, while regression models
monolingual models are trained separately for each language, so they can focus on the specific features of each language. This might lead to better performance in languages with lots of data, because the model can specialize in each language's unique structure, grammar, and vocabulary.
In our experiments, we compare a multilingual model trained on reviews from all languages with separate monolingual models trained on reviews from individual languages. By using the same amount of data for both, we can see how well the multilingual model handles the differences between languages compared to monolingual models that focus on just one language.
sentiment as a continuous variable that preserves the ordering between ratings.

\subsubsection{How much target-language data is needed}
To investigate how model performance scales with the amount of target-language data,
we trained the regression model using progressively larger subsets of the training set,
namely 100k, 200k, 600k, and 1.2M samples.

This experiment aims to identify potential performance plateaus, i.e., points beyond which
adding more training data yields diminishing returns.
Understanding this behavior is essential to balance predictive performance and computational
cost, and to inform the choice of dataset size for subsequent experiments.
In particular, detecting an early plateau allows us to reduce training time and resource
usage while maintaining competitive performance in later cross-lingual analyses.



\subsubsection{How much target-language data is needed}
To investigate how model performance scales with the amount of target-language data,
we trained the regression model using progressively larger subsets of the training set,
namely 100k, 200k, 600k, and 1.2M samples.

This experiment aims to identify potential performance plateaus, i.e., points beyond which
adding more training data yields diminishing returns.
Understanding this behavior is essential to balance predictive performance and computational
cost, and to inform the choice of dataset size for subsequent experiments.
In particular, detecting an early plateau allows us to reduce training time and resource
usage while maintaining competitive performance in later cross-lingual analyses.

\subsubsection{One multilingual model vs. monolingual models: Which works better?}
One of the main challenges in multilingual sentiment analysis is deciding whether to use one multilingual model or separate models for each language. Both options have their pros and cons.\\
A multilingual model is trained on reviews in many languages at once, which helps it learn common patterns across different languages. The big advantage of this approach is that it can handle multiple languages with just one model, saving resources. Also, multilingual models can benefit from cross-lingual transfer, meaning that knowledge learned in one language can help improve performance in other languages, especially for languages with fewer resources.\\
On the other hand, monolingual models are trained separately for each language, so they can focus on the specific features of each language. This might lead to better performance in languages with lots of data, because the model can specialize in each language's unique structure, grammar, and vocabulary.
In our experiments, we compare a multilingual model trained on reviews from all languages with separate monolingual models trained on reviews from individual languages. By using the same amount of data for both, we can see how well the multilingual model handles the differences between languages compared to monolingual models that focus on just one language.
The data is split as follows:\\
  - For the monolingual model, 200k reviews are selected from a single language (e.g., English) and used to train the model.\\
  - For the multilingual model, 200k reviews are distributed across multiple languages (e.g., English, Spanish, French, and German) to train the model.\\
The same model architecture is used for both types of models: distilbert-base-multilingual-cased, a transformer-based model. This model has been pre-trained on a multilingual corpus and fine-tuned for the task of sentiment analysis.

\subsubsection{Per-language performance ranking}
\todo{HIMANSHU or ALBERTO}


% =========================================================
% 3. Results
% =========================================================
\newpage
\noindent\rule{\linewidth}{0.4pt}
\section{Results}

\subsubsection{Regression vs Classification}

Table~\ref{tab:regression_vs_classification} shows that the overall performance of the regression and classification approaches is very similar across most evaluation metrics. In particular, metrics that capture ordinal consistency and ranking quality, such as Quadratic Weighted Kappa (QWK) and Spearman correlation, exhibit only marginal differences between the two settings.

These similarities are also reflected during inference, where both models generally predict comparable star ratings for clearly positive or negative reviews. However, regression achieves slightly higher QWK and Spearman scores, suggesting a better preservation of the ordinal structure of the rating scale.

For this reason, despite the comparable classification-oriented metrics (Accuracy and F1), we ultimately chose the regression formulation. This choice is motivated by its ability to produce continuous scores that better capture sentiment intensity and reduce the impact of small ordinal errors (e.g., predicting 4 instead of 5 stars), which are penalized less severely by ordinal metrics.

\begin{table}[H]
\centering
\rowcolors{2}{white}{white}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Regression} & \textbf{Classification} \\
\midrule
Dataset size &
1.2M &
1.2M \\

MAE $\downarrow$ &
\cellcolor{lightgreen}0.508 &
-- \\

RMSE $\downarrow$ &
\cellcolor{lightgreen}0.692 &
-- \\

QWK $\uparrow$ &
\cellcolor{lightgreen}0.854 &
0.853 \\

Spearman $\uparrow$ &
\cellcolor{lightgreen}0.855 &
0.854 \\

F1 (5-class) $\uparrow$ &
0.740 &
\cellcolor{lightgreen}0.614 \\

F1 (3-class) $\uparrow$ &
0.603 &
\cellcolor{lightgreen}0.746 \\

Accuracy (5-class) $\uparrow$ &
0.596 &
\cellcolor{lightgreen}0.616 \\

Accuracy (3-class) $\uparrow$ &
0.784 &
\cellcolor{lightgreen}0.792 \\

\bottomrule
\end{tabular}
\caption{Comparison between regression and classification}
\label{tab:regression_vs_classification}
\end{table}

\subsubsection{Qualitative Comparison}

The following examples illustrate the behavior of the two approaches at inference time.

\paragraph{Regression}
\begin{itemize}
\item \textbf{Review: VALID} — ``This product is very valid, I suggest this to everyone'' \
Predicted score (normalized): 4.83 \
Predicted stars (rounded): 5/5

\item \textbf{Review: NOT VALID} — ``Pls don't buy this product, it is a scam'' \
Predicted score (normalized): 1.01 \
Predicted stars (rounded): 1/5

\item \textbf{Review: IT OKS} — ``This product has a nice cost/quality rate even if it is not the best one'' \
Predicted score (normalized): 3.68 \
Predicted stars (rounded): 4/5

\item \textbf{Review: DISSAPOINTED} — ``This product is not good. I could say that it is acceptable only because it is very cheap.'' \
Predicted score (normalized): 1.27 \
Predicted stars (rounded): 1/5
\end{itemize}

\paragraph{Classification}
\begin{itemize}
\item \textbf{Review: VALID} \
Predicted stars: 5/5 \
Confidence (softmax): 0.66

\item \textbf{Review: NOT VALID} \
Predicted stars: 1/5 \
Confidence (softmax): 0.99

\item \textbf{Review: IT OKS} \
Predicted stars: 3/5 \
Confidence (softmax): 0.47

\item \textbf{Review: DISSAPOINTED} \
Predicted stars: 1/5 \
Confidence (softmax): 0.80
\end{itemize}

These examples highlight how regression naturally captures intermediate sentiment levels (e.g., borderline positive reviews), while classification tends to force discrete decisions even when confidence is relatively low.

\subsubsection{Results about dataset size}

From Table~\ref{tab:best_dataset_size}, a consistent improvement across all metrics can be observed as the dataset size increases. No clear performance plateau is reached within the explored range, indicating that reducing the dataset size leads to faster training but inevitably results in a loss of predictive quality.

\begin{table}[H]
\centering
\rowcolors{2}{white}{white}
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset size} & \textbf{Epochs} & \textbf{MAE $\downarrow$} & \textbf{RMSE $\downarrow$} & \textbf{QWK $\uparrow$} & \textbf{Acc (5) $\uparrow$} & \textbf{Acc (3) $\uparrow$} \\
\midrule
100k   & 4 & 0.600 & 0.791 & 0.813 & 0.527 & 0.743 \\
200k   & 5 & 0.554 & 0.754 & 0.835 & 0.568 & 0.766 \\
600k   & 4 & 0.526 & 0.710 & 0.848 & 0.585 & 0.778 \\
1.2M   & 4 & \cellcolor{lightgreen}0.508 & \cellcolor{lightgreen}0.692 & \cellcolor{lightgreen}0.854 & \cellcolor{lightgreen}0.596 & \cellcolor{lightgreen}0.784 \\
\bottomrule
\end{tabular}
\caption{Impact of dataset size on regression performance}
\label{tab:best_dataset_size}
\end{table}

Moreover, the best values for all major metrics are already achieved between 4 and 5 epochs. Extending training beyond this range does not provide additional benefits and may increase the risk of overfitting while unnecessarily increasing computational cost. For this reason, early stopping around 4 epochs is recommended, rather than training up to 8 epochs (Figure \ref{fig:epoch_comparison}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{epoch_comparison.png}
    \caption{Epoch related to the dataset size}
    \label{fig:epoch_comparison}
\end{figure}

Overall, these results suggest a trade-off between efficiency and performance: smaller datasets significantly reduce training time but degrade ordinal consistency and accuracy, while larger datasets continue to yield measurable gains without exhibiting saturation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{qwk_regression.png}
    \caption{QWK metrics}
    \label{fig:qwk_regression}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{spearmanr_regression.png}
    \caption{Spearman metrics}
    \label{fig:spearman_regression}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{rmse_regression.png}
    \caption{RMSE metrics}
    \label{fig:rmse_regression}
\end{figure}


\subsubsection{One multilingual model vs. monolingual models}


As expected, the single-language model shows faster training times compared to the multilingual model. 
This is because the single-language model is trained on data that is more homogeneous and specific to a single language (in this case German). 
The reviews in this dataset share more similarities, which allows the model to focus on language-specific features without the added complexity of handling multiple languages.
On the other hand, the multilingual model has to process reviews in four different languages (English, Spanish, French, and German, Chinese), which introduces additional challenges. 
The model needs to generalize across diverse linguistic structures, vocabulary, and syntactic differences, making the training process slower as it deals with a broader and more complex set of data. 
The training loss for both models over time is shown in Figure \ref{fig:train_loss}, where we can observe the faster convergence of the single-language model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{loss1.png}
    \caption{Training loss comparison between multilingual and monolingual models. The multilingual model (in pink) shows slower convergence compared to the single-language model (in purple).}
    \label{fig:train_loss}
\end{figure}


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Metric Training} & \textbf{Multilingual Model} & \textbf{Single Language Model (English)} \\
\midrule
Dataset & Multilingual (English, Spanish, French, German, Chinese) & Single Language (German) \\
Dataset Size & 200k (balanced across languages) & 200k (German-only) \\
Train Loss & 0.3909 & 0.3543 \\
Eval Loss & 0.524 & 0.483 \\
Accuracy Eval(3-class) & 0.770 & 0.748 \\
Macro F1 Eval (3-class) & 0.770 & 0.792 \\
Macro Precision Eval &  0.715 & 0.758 \\
Macro Recall Eval & 0.706 & 0.745 \\
Training Time (hours) & 4 & 4 \\
Number of Steps & 13500 & 13500 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of Multilingual and Single Language (German) Models on the Same Dataset (either Multilingual or English-only).}
\label{tab:model_comparison}
\end{table}


\paragraph{Model Evaluation on German Test Set}

Both models are tested on a set of 5000 German reviews. 
The results show that the single-language model performs slightly better than the multilingual model, as expected. 
The accuracy of the single-language model is 81.34, higher than the 80.22 of the multilingual model.

 
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Metric Evaluation} & \textbf{Multilingual Model (DE)} & \textbf{Single Language Model (DE)} \\
\midrule
Accuracy & 0.8022 & 0.8134 \\
Precision & 0.7596 & 0.7701 \\
Recall & 0.7550 & 0.7563 \\
F1 Score & 0.7566 & 0.7597 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of Multilingual and Single Language Models Evaluated on a Test Set of 5000 German Reviews.}
\label{tab:test_set_comparison}
\end{table}



\paragraph{Model Evaluation on Cross-Lingual Test Set}

Both models are tested on a cross-lingual test set with 1000 reviews in five languages: Japanese (ja), Spanish (es), French (fr), Chinese (zh), and English (en). 
The results show that the single-language model, trained only on German data, performs much worse on this mixed-language dataset. 
The accuracy of the single-language model is much lower than the accuracy of the multilingual model.
Looking at the error distribution (shown in the bar plots), we can see that the single-language model makes fewer mistakes in English and Spanish, as these languages are more similar to German. 
However, it struggles more with reviews in Japanese, French, and Chinese, as it was not trained on these languages. 
These results show that while the single-language model works well for German, it struggles with data in other languages. The multilingual model, however, is more flexible and works better with data in multiple languages. To compare the models more fully, it would be useful to test them on other diverse datasets or in real-world multilingual situations.


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcc}
\toprule
\textbf{Metric Evaluation} & \textbf{Multilingual Model} & \textbf{Single Language Model} \\
\midrule
Accuracy & 0.7650 & 0.6110 \\
Precision & 0.7075 & 0.5658 \\
Recall & 0.6945 & 0.5417 \\
F1 Score & 0.6966 & 0.5390 \\
\bottomrule
\end{tabular}
}
\caption{Comparison of Multilingual and Single Language Models Evaluated on a Cross-Lingual Test Set (1000 Reviews, Excluding German).}
\label{tab:cross_lingual_comparison}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{errors_by_language_single.png} % replace with your image path
    \caption{Errors by language for the Single Language Model.}
    \label{fig:single_language_errors}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{errors_by_language_multilingual.png} % replace with your image path
    \caption{Errors by language for the Multilingual Model.}
    \label{fig:multilingual_errors}
\end{figure}

% =========================================================
% 4. Discussion
% =========================================================
\newpage
\section{Discussion}

\subsection{Key findings}
Our baseline confirms that a multilingual encoder fine-tuned as a regressor can capture sentiment intensity and ordinal structure... 

\paragraph{Multilingual vs Monolingual Models}
In this study, we compared a single-language model and a multilingual model. 
The results show that each model has its strengths depending on the task.
The single-language model works very well when the data is in one language, as it is specifically trained for that language. 
However, it has difficulties when working with languages it wasn’t trained on, like Japanese, Spanish, or Chinese. 
This makes it less suitable for tasks involving multiple languages.
The multilingual model, on the other hand, performs well across different languages. 
While it wasn’t as strong as the single-language model on German reviews, it performed more consistently on the cross-lingual test set. 
It is more flexible and is a better choice when the task involves multiple languages or when data comes from different linguistic backgrounds.
In summary, if you’re working with data in just one language, the single-language model is probably the best choice. But if you need to handle reviews or text in multiple languages, the multilingual model is more versatile and offers better performance across languages.

% =========================================================
% 5. Conclusion
% =========================================================
\newpage
\section{Conclusion}
We presented a baseline pipeline for multilingual sentiment prediction on Amazon Reviews and validated training efficiency on a GPU cluster.
Using a multilingual DistilBERT encoder, the regression setup achieves strong ordinal agreement (QWK) and rank preservation (Spearman),
supporting the feasibility of larger-scale cross-lingual experiments.
Future work will systematically evaluate data requirements per language and compare multilingual vs monolingual training strategies.

% =========================================================
% References
% =========================================================
\clearpage
\begin{thebibliography}{9}

\bibitem{devlin2019bert}
Devlin J., Chang M.-W., Lee K., Toutanova K.,
\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
\textit{Proceedings of NAACL-HLT}, 2019, pp. 4171--4186.

\bibitem{sanh2019distilbert}
Sanh V., Debut L., Chaumond J., Wolf T.,
\textit{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
\textit{arXiv preprint} arXiv:1910.01108, 2019.

\bibitem{wolf2020transformers}
Wolf T., Debut L., Sanh V., et al.,
\textit{Transformers: State-of-the-Art Natural Language Processing},
\textit{Proceedings of EMNLP: System Demonstrations}, 2020, pp. 38--45.

\bibitem{wandb}
Biewald L.,
\textit{Experiment Tracking with Weights and Biases},
\textit{Software available from wandb.com}, 2020.

\bibitem{kaggle_amazon_multilingual}
Mexwell (Kaggle uploader),
\textit{Amazon Reviews Multi (Kaggle dataset)},
\textit{Kaggle}, year unknown.
\url{https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi/data}

\bibitem{hf_distilbert_multilingual}
Hugging Face,
\textit{distilbert-base-multilingual-cased: Model card},
Hugging Face Model Hub, 2019.
Available at: \url{https://huggingface.co/distilbert-base-multilingual-cased}

\end{thebibliography}

% =========================================================
% Appendix A: Reflection with respect to learning objectives
% =========================================================
\appendix
\section{Reflection with respect to learning objectives}
This project aligns with the course learning objectives by requiring:
\begin{itemize}[itemsep=2pt]
    \item Understanding and application of core NLP building blocks, including tokenization, embeddings, and Transformer architectures (attention mechanisms) as the foundation of modern language understanding systems;
    \item Implementation and fine-tuning of pre-trained language models (e.g., with Hugging Face and/or PyTorch) to solve practical NLP tasks such as text classification and sentiment analysis (and, where applicable, sequence-to-sequence tasks like translation);
    \item Comparison of alternative modeling choices by analyzing evaluation metrics and qualitative outputs, assessing their effectiveness for language modeling and sequence generation behavior where relevant;
    \item Critical assessment of strengths and limitations of different NLP approaches, considering model capacity, scalability, computational cost, and robustness to real-world, domain-specific language variation;
    \item Development of an end-to-end, scalable NLP solution that integrates preprocessing, embeddings, and advanced architectures, with attention to deployment considerations and reproducible experimentation.
\end{itemize}


% =========================================================
% Appendix B: Work tasks distribution
% =========================================================
\section{Work tasks distribution}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm}p{11cm}}
\toprule
\textbf{Member} & \textbf{Main contributions} \\
\midrule
Imanshu Sharma & Per-Language performance ranking \\
Alberto Aiello & Multilingual model vs Singlelingual model, dataset preprocessing, training pipeline, inference, report \\
Luca Anzaldi & Regression setup, metrics (QWK/Spearman), training pipeline, test on different dataset size, report \\
\bottomrule
\end{tabular}
\caption{Work distribution}
\end{table}

\end{document}

