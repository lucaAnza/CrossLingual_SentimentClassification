\documentclass[11pt]{article}

% ---------- Packages ----------
\usepackage[a4paper,margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{todonotes}
\definecolor{lightgreen}{RGB}{220,245,220}  % Used for table highlights

% ---------- Title ----------
\title{\textbf{Sentiment Analysis of Multilingual Amazon Reviews}\\
AI509/508/DM894 Final Project Report}
\author{Imanshu Sharma \and Alberto Aiello \and Luca Anzaldi}
\date{\today}

\begin{document}
\maketitle

% Optional (not required by template, but commonly useful)
\begin{abstract}
This project investigates sentiment prediction on a multilingual Amazon Reviews dataset.
We study (i) how much target-language data is needed to reach near-peak performance,
(ii) whether a single multilingual model trained on all languages outperforms monolingual
models trained per language, and (iii) the per-language ranking of performance.
We present an initial baseline experiment using a compact multilingual encoder model to
validate the pipeline and assess training efficiency.
\end{abstract}

% =========================================================
% 1. Introduction and Problem Presentation
% =========================================================
\section{Introduction and Problem Presentation}

\subsection{Task description}
Given an Amazon review text, the goal is to predict its sentiment.
We consider two related formulations:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Classification:} predict discrete sentiment labels (e.g., negative / neutral / positive or 1--5 stars).
    \item \textbf{Regression:} predict a continuous sentiment score derived from the 1--5 star rating.
\end{itemize}
In later experiments, we focus on cross-lingual behavior and compare performance across languages.

\subsection{Project goal and research questions}
Our project investigates the following questions:
\begin{enumerate}[itemsep=2pt]
    \item \textbf{How much target-language data is needed?}
    We vary training set size (e.g., 10\%, 30\%, 50\%, 70\%, 100\%) and identify where performance saturates.
    \item \textbf{One multilingual model vs. monolingual models: which works better?}
    We compare a shared multilingual model trained on all languages against separate models trained per language.
    \item \textbf{Per-language performance ranking:}
    We compute and compare evaluation metrics for each language to understand which languages are easier/harder.
\end{enumerate}

\subsection{Why the problem is challenging}
\begin{itemize}[itemsep=2pt]
    \item \textbf{Noisy labels:} star ratings may not perfectly reflect sentiment in the text.
    \item \textbf{Ordinal target:} 1--5 stars are ordered categories; mistakes are not equally severe (1$\rightarrow$2 vs 1$\rightarrow$5).
    \item \textbf{Multilingual variability:} languages differ in script, morphology, and tokenization behavior.
    \item \textbf{Domain shift across categories:} product categories and writing style may vary significantly.
\end{itemize}

% =========================================================
% 2. Method
% =========================================================
\newpage
\noindent\rule{\linewidth}{0.4pt}
\section{Method}

\subsection{Computing resources (Hardware)}
All experiments were executed on the SDU/University GPU cluster. In the table below are showed the \textit{hardware specification}.

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Compute node & \texttt{u1-gpu-h-1} \\
CPU & Intel Xeon Gold 6230 (20 vCPUs) \\
RAM & 46 GB \\
GPU & 1 $\times$ NVIDIA V100 \\
OS / Environment & Linux + Conda \\
\bottomrule
\end{tabular}
\caption{Hardware configuration used for model training and evaluation.}
\label{tab:hardware}
\end{table}

\subsection{Software stack (Frameworks and tools)}
We implemented the pipeline in Python using the following libraries:
\begin{itemize}[itemsep=2pt]
    \item \textbf{PyTorch}: backend for model training and automatic differentiation.
    \item \textbf{Hugging Face Transformers}: model loading, tokenization, and the training loop (\texttt{Trainer}).
    \item \textbf{Hugging Face Datasets}: loading CSV splits and preprocessing with efficient mapping.
    \item \textbf{Evaluate}: metric computation during evaluation.
    \item \textbf{Weights \& Biases (wandb)}: experiment tracking (loss curves, metrics, run metadata).
    \item \textbf{NumPy}: metric utilities and post-processing.
\end{itemize}

\subsection{Dataset overview}
We use a multilingual Amazon Reviews dataset \cite{kaggle_amazon_multilingual} split into \texttt{train.csv}, \texttt{validation.csv}, and \texttt{test.csv}.
Each review includes a 1--5 star rating and textual fields (title/body), along with metadata such as language and product category.
The dataset covers multiple languages (e.g., EN, ES, DE, FR, JA, ZH).

For the baseline experiments, we subsample the dataset to enable faster experimentation while preserving class balance.
Specifically, we use up to 1.2M training samples and cap validation and test splits at 30k samples each.

\subsubsection{Label distribution}
The training split is perfectly balanced across the five star ratings, as shown in Table~\ref{tab:label_distribution_train}.

\begin{table}[H]
\centering
\begin{tabular}{cc}
\toprule
\textbf{Star rating} & \textbf{Number of samples} \\
\midrule
1 & 240{,}000 \\
2 & 240{,}000 \\
3 & 240{,}000 \\
4 & 240{,}000 \\
5 & 240{,}000 \\
\bottomrule
\end{tabular}
\caption{Label distribution in the training set (balanced).}
\label{tab:label_distribution_train}
\end{table}

The validation and test sets follow the same balanced distribution, with 6{,}000 samples per star rating.

\subsubsection{Dataset split sizes}
Table~\ref{tab:dataset_splits} summarizes the number of samples used in each split.

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Split} & \textbf{Samples per class} & \textbf{Total samples} \\
\midrule
Training & 240{,}000 & 1{,}200{,}000 \\
Validation & 6{,}000 & 30{,}000 \\
Test & 6{,}000 & 30{,}000 \\
\bottomrule
\end{tabular}
\caption{Overview of dataset splits used in the experiments.}
\label{tab:dataset_splits}
\end{table}

This balanced setup allows a fair comparison between regression and classification approaches
and ensures that evaluation metrics are not biased by class imbalance.

\subsection{Model choice}
We use \texttt{distilbert-base-multilingual-cased}, an \textbf{encoder-only} Transformer model.
It is a distilled variant of multilingual BERT, providing a good trade-off between computational
efficiency and multilingual coverage \cite{sanh2019distilbert,wolf2020transformers}.\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Feature} & \textbf{Specification} \\
\midrule
Model name & \texttt{distilbert-base-multilingual-cased} \\
Architecture & Encoder-only Transformer (DistilBERT) \\
Number of layers & 6 Transformer encoder layers \\
Hidden size & 768 \\
Attention heads & 12 \\
Total parameters & $\sim$134 million \\
Vocabulary size & $\sim$119{,}547 \\
Maximum sequence length & 512 tokens \\
Pretraining objective & Masked Language Modeling (MLM) \\
Languages covered & 100+ languages \\
Case sensitive & Yes (cased) \\
Disk size (weights) & $\sim$517 MB \\
\bottomrule
\end{tabular}
\caption{Main characteristics of the \texttt{distilbert-base-multilingual-cased} model \cite{hf_distilbert_multilingual}}
\label{tab:model_features}
\end{table}

\subsection{Preprocessing and input representation}
\paragraph{Column selection.}\label{column-selection}
We rename \texttt{stars} to \texttt{label} and construct the input text by concatenating the review title and body
(\texttt{text} = \texttt{review\_title} + \texttt{review\_body}).
Non-essential metadata (e.g., review IDs, product identifiers, language tags, and product category fields)
are removed for the baseline experiments.


\paragraph{Tokenization.}
We tokenize the new \texttt{text} column (prodecure on \ref{column-selection}) with the model tokenizer using truncation and dynamic padding (via \texttt{DataCollatorWithPadding}).

\paragraph{Label normalization.}
In the \textit{regression} setting, the original star ratings
$y \in \{1,2,3,4,5\}$ are linearly normalized to the interval $[0,1]$:
\[
y_{\text{norm}} = \frac{y - 1}{4}.
\]
For evaluation and interpretability, model predictions are mapped back to the original
rating scale using the inverse transformation:
\[
\hat{y} = 4 \cdot \hat{y}_{\text{norm}} + 1.
\]

In the \textit{classification setting}, star ratings are re-encoded from the original range
$\{1,\dots,5\}$ to $\{0,\dots,4\}$, in accordance with the Hugging Face
\texttt{Transformers} framework, which requires class labels to be zero-indexed.


%%% TRAINING SETUP
\subsection{Training setup}
Training is performed using task-specific loss functions.
For the \textit{regression formulation}, we minimize the \textbf{Mean Squared Error (MSE)} loss between
predicted and ground-truth star ratings.
For the \textit{classification formulation}, we optimize the \textbf{categorical cross-entropy} loss,
as standard for multi-class classification in the Hugging Face \texttt{Transformers} framework.

%Table with settings
\begin{table}[H]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Training batch size & 32 \\
Gradient accumulation steps & 4 \\
Effective batch size & 128 \\
Number of epochs & 8 \\
Weight decay & 0.01 \\
Learning rate scheduler & Cosine \\
Warmup ratio & 0.06 \\
Maximum gradient norm & 1.0 \\
\bottomrule
\end{tabular}
\caption{Key training hyperparameters used in the experiments.}
\label{tab:training_hyperparameters}
\end{table}

% Hyperparams motivation
\paragraph{Hyperparameter selection.}
\begin{itemize}[itemsep=2pt]
    \item The training batch size was set to 32 to ensure stable optimization while avoiding GPU memory constraints.
    \item Gradient accumulation with four steps was used to simulate a larger effective batch size of 128 without increasing memory usage.
    \item The model was trained for eight epochs to allow sufficient convergence while limiting the risk of overfitting.
    \item Weight decay was applied to regularize model parameters and mitigate overfitting on noisy sentiment labels.
    \item A cosine learning rate scheduler with a warmup ratio of 0.06 was adopted to ensure smooth learning rate decay and stable early training.
    \item Gradient clipping with a maximum norm of 1.0 was employed to prevent exploding gradients and improve overall training stability.
\end{itemize}


\subsection{Evaluation metrics}
Evaluation is performed using different metrics depending on whether the task is formulated
as \textbf{regression} or \textbf{classification}.

\subsubsection{Regression setting.}
When modeling sentiment as a continuous variable, we report the following metrics:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Mean Absolute Error (MAE)} and \textbf{Root Mean Squared Error (RMSE)}, computed in the original 1--5 star scale, to measure average prediction error.
    \item \textbf{Rounded Accuracy (5-class)}, obtained by rounding the continuous prediction to the nearest integer star rating.
    \item \textbf{Rounded Accuracy (3-class)}, computed after mapping 1--2$\rightarrow$negative, 3$\rightarrow$neutral, and 4--5$\rightarrow$positive.
    \item \textbf{Spearman’s rank correlation coefficient ($\rho$)}, to assess how well the model preserves the ordinal ranking of sentiment.
    \item \textbf{Quadratic Weighted Kappa (QWK)}, which measures ordinal agreement while penalizing larger rating discrepancies more heavily.
\end{itemize}

\subsubsection{Classification setting.}
When sentiment prediction is formulated as a classification task, we report:
\begin{itemize}[itemsep=2pt]
    \item \textbf{Accuracy (5-class)} and \textbf{Macro F1-score (5-class)}, to evaluate exact label prediction performance across all star ratings.
    \item \textbf{Quadratic Weighted Kappa (QWK)}, to account for the ordinal structure of the 1--5 star labels.
    \item \textbf{Accuracy (3-class)} and \textbf{Macro F1-score (3-class)}, computed after mapping ratings to negative, neutral, and positive sentiment.
\end{itemize}


\subsection{Experiment protocol}

\subsubsection{Regression vs Classification}
The first set of experiments was designed to assess whether sentiment prediction should be
formulated as a \textit{regression} or a \textit{classification} problem.
Since the Amazon review ratings are ordinal in nature, both approaches are viable:
classification treats each star rating as a discrete category, while regression models
sentiment as a continuous variable that preserves the ordering between ratings.

\subsubsection{How much target-language data is needed}
To investigate how model performance scales with the amount of target-language data,
we trained the regression model using progressively larger subsets of the training set,
namely 100k, 200k, 600k, and 1.2M samples.

This experiment aims to identify potential performance plateaus, i.e., points beyond which
adding more training data yields diminishing returns.
Understanding this behavior is essential to balance predictive performance and computational
cost, and to inform the choice of dataset size for subsequent experiments.
In particular, detecting an early plateau allows us to reduce training time and resource
usage while maintaining competitive performance in later cross-lingual analyses.

\subsubsection{One multilingual model vs. monolingual models}
\todo{HIMANSHU or ALBERTO}
\subsubsection{Per-language performance ranking}
\todo{HIMANSHU or ALBERTO}


% =========================================================
% 3. Results
% =========================================================
\newpage
\noindent\rule{\linewidth}{0.4pt}
\section{Results}

\subsubsection{Regression vs Classification}

Table~\ref{tab:regression_vs_classification} shows that the overall performance of the regression and classification approaches is very similar across most evaluation metrics. In particular, metrics that capture ordinal consistency and ranking quality, such as Quadratic Weighted Kappa (QWK) and Spearman correlation, exhibit only marginal differences between the two settings.

These similarities are also reflected during inference, where both models generally predict comparable star ratings for clearly positive or negative reviews. However, regression achieves slightly higher QWK and Spearman scores, suggesting a better preservation of the ordinal structure of the rating scale.

For this reason, despite the comparable classification-oriented metrics (Accuracy and F1), we ultimately chose the regression formulation. This choice is motivated by its ability to produce continuous scores that better capture sentiment intensity and reduce the impact of small ordinal errors (e.g., predicting 4 instead of 5 stars), which are penalized less severely by ordinal metrics.

\begin{table}[H]
\centering
\rowcolors{2}{white}{white}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Regression} & \textbf{Classification} \\
\midrule
Dataset size &
1.2M &
1.2M \\

MAE $\downarrow$ &
\cellcolor{lightgreen}0.508 &
-- \\

RMSE $\downarrow$ &
\cellcolor{lightgreen}0.692 &
-- \\

QWK $\uparrow$ &
\cellcolor{lightgreen}0.854 &
0.853 \\

Spearman $\uparrow$ &
\cellcolor{lightgreen}0.855 &
0.854 \\

F1 (5-class) $\uparrow$ &
0.740 &
\cellcolor{lightgreen}0.614 \\

F1 (3-class) $\uparrow$ &
0.603 &
\cellcolor{lightgreen}0.746 \\

Accuracy (5-class) $\uparrow$ &
0.596 &
\cellcolor{lightgreen}0.616 \\

Accuracy (3-class) $\uparrow$ &
0.784 &
\cellcolor{lightgreen}0.792 \\

\bottomrule
\end{tabular}
\caption{Comparison between regression and classification}
\label{tab:regression_vs_classification}
\end{table}

\subsubsection{Qualitative Comparison}

The following examples illustrate the behavior of the two approaches at inference time.

\paragraph{Regression}
\begin{itemize}
\item \textbf{Review: VALID} — ``This product is very valid, I suggest this to everyone'' \
Predicted score (normalized): 4.83 \
Predicted stars (rounded): 5/5

\item \textbf{Review: NOT VALID} — ``Pls don't buy this product, it is a scam'' \
Predicted score (normalized): 1.01 \
Predicted stars (rounded): 1/5

\item \textbf{Review: IT OKS} — ``This product has a nice cost/quality rate even if it is not the best one'' \
Predicted score (normalized): 3.68 \
Predicted stars (rounded): 4/5

\item \textbf{Review: DISSAPOINTED} — ``This product is not good. I could say that it is acceptable only because it is very cheap.'' \
Predicted score (normalized): 1.27 \
Predicted stars (rounded): 1/5
\end{itemize}

\paragraph{Classification}
\begin{itemize}
\item \textbf{Review: VALID} \
Predicted stars: 5/5 \
Confidence (softmax): 0.66

\item \textbf{Review: NOT VALID} \
Predicted stars: 1/5 \
Confidence (softmax): 0.99

\item \textbf{Review: IT OKS} \
Predicted stars: 3/5 \
Confidence (softmax): 0.47

\item \textbf{Review: DISSAPOINTED} \
Predicted stars: 1/5 \
Confidence (softmax): 0.80
\end{itemize}

These examples highlight how regression naturally captures intermediate sentiment levels (e.g., borderline positive reviews), while classification tends to force discrete decisions even when confidence is relatively low.

\subsubsection{Results about dataset size}

From Table~\ref{tab:best_dataset_size}, a consistent improvement across all metrics can be observed as the dataset size increases. No clear performance plateau is reached within the explored range, indicating that reducing the dataset size leads to faster training but inevitably results in a loss of predictive quality.

\begin{table}[H]
\centering
\rowcolors{2}{white}{white}
\begin{tabular}{lcccccc}
\toprule
\textbf{Dataset size} & \textbf{Epochs} & \textbf{MAE $\downarrow$} & \textbf{RMSE $\downarrow$} & \textbf{QWK $\uparrow$} & \textbf{Acc (5) $\uparrow$} & \textbf{Acc (3) $\uparrow$} \\
\midrule
100k   & 4 & 0.600 & 0.791 & 0.813 & 0.527 & 0.743 \\
200k   & 5 & 0.554 & 0.754 & 0.835 & 0.568 & 0.766 \\
600k   & 4 & 0.526 & 0.710 & 0.848 & 0.585 & 0.778 \\
1.2M   & 4 & \cellcolor{lightgreen}0.508 & \cellcolor{lightgreen}0.692 & \cellcolor{lightgreen}0.854 & \cellcolor{lightgreen}0.596 & \cellcolor{lightgreen}0.784 \\
\bottomrule
\end{tabular}
\caption{Impact of dataset size on regression performance}
\label{tab:best_dataset_size}
\end{table}

Moreover, the best values for all major metrics are already achieved between 4 and 5 epochs. Extending training beyond this range does not provide additional benefits and may increase the risk of overfitting while unnecessarily increasing computational cost. For this reason, early stopping around 4 epochs is recommended, rather than training up to 8 epochs (Figure \ref{fig:epoch_comparison}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{epoch_comparison.png}
    \caption{Epoch related to the dataset size}
    \label{fig:epoch_comparison}
\end{figure}

Overall, these results suggest a trade-off between efficiency and performance: smaller datasets significantly reduce training time but degrade ordinal consistency and accuracy, while larger datasets continue to yield measurable gains without exhibiting saturation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{qwk_regression.png}
    \caption{QWK metrics}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{spearmanr_regression.png}
    \caption{Spearman metrics}
    \label{fig:placeholder}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{rmse_regression.png}
    \caption{RMSE metrics}
    \label{fig:placeholder}
\end{figure}





% =========================================================
% 4. Discussion
% =========================================================
\newpage
\section{Discussion}

\subsection{Key findings}
Our baseline confirms that a multilingual encoder fine-tuned as a regressor can capture sentiment intensity and ordinal structure... 
\todo{Finish this chapter using the result of question2 and question3}


% =========================================================
% 5. Conclusion
% =========================================================
\newpage
\section{Conclusion}
We presented a baseline pipeline for multilingual sentiment prediction on Amazon Reviews and validated training efficiency on a GPU cluster.
Using a multilingual DistilBERT encoder, the regression setup achieves strong ordinal agreement (QWK) and rank preservation (Spearman),
supporting the feasibility of larger-scale cross-lingual experiments.
Future work will systematically evaluate data requirements per language and compare multilingual vs monolingual training strategies.

% =========================================================
% References
% =========================================================
\clearpage
\begin{thebibliography}{9}

\bibitem{devlin2019bert}
Devlin J., Chang M.-W., Lee K., Toutanova K.,
\textit{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
\textit{Proceedings of NAACL-HLT}, 2019, pp. 4171--4186.

\bibitem{sanh2019distilbert}
Sanh V., Debut L., Chaumond J., Wolf T.,
\textit{DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
\textit{arXiv preprint} arXiv:1910.01108, 2019.

\bibitem{wolf2020transformers}
Wolf T., Debut L., Sanh V., et al.,
\textit{Transformers: State-of-the-Art Natural Language Processing},
\textit{Proceedings of EMNLP: System Demonstrations}, 2020, pp. 38--45.

\bibitem{wandb}
Biewald L.,
\textit{Experiment Tracking with Weights and Biases},
\textit{Software available from wandb.com}, 2020.

\bibitem{kaggle_amazon_multilingual}
Mexwell (Kaggle uploader),
\textit{Amazon Reviews Multi (Kaggle dataset)},
\textit{Kaggle}, year unknown.
\url{https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi/data}

\bibitem{hf_distilbert_multilingual}
Hugging Face,
\textit{distilbert-base-multilingual-cased: Model card},
Hugging Face Model Hub, 2019.
Available at: \url{https://huggingface.co/distilbert-base-multilingual-cased}

\end{thebibliography}

% =========================================================
% Appendix A: Reflection with respect to learning objectives
% =========================================================
\appendix
\section{Reflection with respect to learning objectives}
This project aligns with the course learning objectives by requiring:
\begin{itemize}[itemsep=2pt]
    \item Understanding and application of core NLP building blocks, including tokenization, embeddings, and Transformer architectures (attention mechanisms) as the foundation of modern language understanding systems;
    \item Implementation and fine-tuning of pre-trained language models (e.g., with Hugging Face and/or PyTorch) to solve practical NLP tasks such as text classification and sentiment analysis (and, where applicable, sequence-to-sequence tasks like translation);
    \item Comparison of alternative modeling choices by analyzing evaluation metrics and qualitative outputs, assessing their effectiveness for language modeling and sequence generation behavior where relevant;
    \item Critical assessment of strengths and limitations of different NLP approaches, considering model capacity, scalability, computational cost, and robustness to real-world, domain-specific language variation;
    \item Development of an end-to-end, scalable NLP solution that integrates preprocessing, embeddings, and advanced architectures, with attention to deployment considerations and reproducible experimentation.
\end{itemize}


% =========================================================
% Appendix B: Work tasks distribution
% =========================================================
\section{Work tasks distribution}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm}p{11cm}}
\toprule
\textbf{Member} & \textbf{Main contributions} \\
\midrule
Imanshu Sharma & TODO \\
Alberto Aiello & TODO \\
Luca Anzaldi & Regression setup, metrics (QWK/Spearman), training pipeline, test on different dataset size, report \\
\bottomrule
\end{tabular}
\caption{Work distribution}
\end{table}

\end{document}

